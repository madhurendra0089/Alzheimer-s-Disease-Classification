{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5e762bf-bf9b-41e1-a994-5a9e3f92a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 4.2599\n",
      "Epoch [2/5], Loss: 4.0605\n",
      "Epoch [3/5], Loss: 3.8662\n",
      "Epoch [4/5], Loss: 3.6782\n",
      "Epoch [5/5], Loss: 3.4979\n",
      "GPT2Model(\n",
      "  (token_embedding): Embedding(65, 256)\n",
      "  (positional_embedding): Embedding(512, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x GPT2Layer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (values): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (keys): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (queries): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedForward(\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=256, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests\n",
    "\n",
    "class GPT2Layer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden_dim):\n",
    "        super(GPT2Layer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.feedforward = FeedForward(embed_size, ff_hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention_output = self.attention(value, key, query, mask)\n",
    "        x = self.norm1(attention_output + query)\n",
    "        forward_output = self.feedforward(x)\n",
    "        out = self.norm2(forward_output + x)\n",
    "        return out\n",
    "\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, num_layers, heads, ff_hidden_dim, device):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embedding = nn.Embedding(max_len, embed_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GPT2Layer(embed_size, heads, ff_hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        out = self.token_embedding(x) + self.positional_embedding(positions)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.to(self.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden_dim)\n",
    "        self.fc2 = nn.Linear(ff_hidden_dim, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=512):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        tokens = [self.vocab[char] for char in text][:self.max_len]\n",
    "        input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "# Download the data\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(data_url)\n",
    "text_data = response.text\n",
    "\n",
    "# Create vocab\n",
    "vocab = {char: idx for idx, char in enumerate(set(text_data))}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "dataset = TextDataset([text_data], vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model and optimizer initialization\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "num_layers = 4\n",
    "ff_hidden_dim = 512\n",
    "max_len = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2Model(vocab_size, embed_size, max_len, num_layers, heads, ff_hidden_dim, device).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "# ...\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_ids in data_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Use input_ids.size(0) instead of input_ids.size(1) for mask creation\n",
    "        mask = torch.triu(torch.ones(input_ids.size(0), input_ids.size(0))).transpose(0, 1).type(dtype=torch.bool)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        # Note: No need to specify mask here, as GPT2 inherently attends to all positions\n",
    "        loss = criterion(outputs.view(-1, vocab_size), input_ids.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96a50ab-3cfb-417d-99ee-d86643aeeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # You can change this to 'gpt2-medium', 'gpt2-large', etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db482184-f8e3-42a1-9219-b5758126fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/miniconda3/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madhu is a good boy. He is very good at his job. I am not sure if he is going to be able to do it.\n",
      "\n",
      "\"I am sure he will be fine. But I don't know if it will work\n"
     ]
    }
   ],
   "source": [
    "# Encode input text\n",
    "input_text = \"madhu is a good boy\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
