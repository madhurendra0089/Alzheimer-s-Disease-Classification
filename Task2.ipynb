{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2**"
      ],
      "metadata": {
        "id": "YrpcbEqFy0C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " * I didn't have enough resources to run with GPT 2 embedding size and max sequence length and other parameters. So i reduced it to smaller parameters.\n",
        " * Task 2: Learnt about Rotary Positional Embedding(Instead of Positional Embedding) and Group Query Attention built(in place of self attention) corresponding function.\n",
        ""
      ],
      "metadata": {
        "id": "v4jFtYAv6H8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rotary Positional Embedding:"
      ],
      "metadata": {
        "id": "4y6A8s6TzOWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the angles:\n",
        "angles = positions.unsqueeze(−1)/10000^((2⋅(freq//2))/embed_size)\n",
        "Here, freq is a tensor containing frequencies, usually with values\n",
        "0,2,4,…,embed_size−2\n",
        "\n",
        "\n",
        "Compute the sine and cosine components:\n",
        "embeddings=concatenate(sin(angles),cos(angles))embeddings=concatenate(sin(angles),cos(angles))\n",
        "The final embeddings tensor will have a shape of (\n",
        "batch_size,sequence_length,embed_size)(batch_size,sequence_length,embed_size)."
      ],
      "metadata": {
        "id": "I6Ro8R7t8SiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def rotary_embedding(positions, embed_size):\n",
        "    freq = torch.arange(0, embed_size, 2).float()\n",
        "    angles = positions.unsqueeze(-1) / torch.pow(10000, (2 * (freq // 2)) / embed_size)\n",
        "    embeddings = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "8SX4-oMMnES2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Query Attention"
      ],
      "metadata": {
        "id": "Hm5xifegzUg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def group_query_attention(x, embed_size, num_groups):\n",
        "    batch_size, seq_length, _ = x.size()\n",
        "\n",
        "    # queries\n",
        "    grouped_queries = x.view(batch_size, seq_length // num_groups, num_groups, embed_size)\n",
        "\n",
        "    # attention\n",
        "    attended_groups = []\n",
        "    for group_idx in range(num_groups):\n",
        "        queries = torch.nn.Linear(embed_size, embed_size)(grouped_queries[:, :, group_idx, :])\n",
        "        keys = torch.nn.Linear(embed_size, embed_size)(x)\n",
        "        values = torch.nn.Linear(embed_size, embed_size)(x)\n",
        "\n",
        "        attention_scores = torch.einsum('bqe,bte->bqt', queries, keys) / (embed_size ** 0.5)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        attended_group = torch.einsum('bqt,bte->bqe', attention_weights, values)\n",
        "        attended_groups.append(attended_group)\n",
        "\n",
        "    # Combine group results\n",
        "    attended_sequence = torch.cat(attended_groups, dim=2)\n",
        "\n",
        "    return attended_sequence"
      ],
      "metadata": {
        "id": "eU6UAiBFm71h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Rd9_NKC8_fr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}